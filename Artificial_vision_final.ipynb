{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNg7o57i8FENY9VuRMuisYO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"Bnu7eBH2i0rW"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"baiB-nSzIiY8"},"outputs":[],"source":["!pip install kaggle"]},{"cell_type":"code","source":["from google.colab import files\n","import random"],"metadata":{"id":"2AHH2xSAI0z1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["files.upload()\n","!mkdir -p ~/.kaggle\n","!cp kaggle.json ~/.kaggle/\n","!chmod 600 ~/.kaggle/kaggle.json"],"metadata":{"id":"frH-OR-wIy_4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !wget -O dataset.zip \"https://www.kaggle.com/datasets/adityajn105/flickr8k/download?datasetVersionNumber=1\"\n","\n","!kaggle datasets download -d \"adityajn105/flickr8k\""],"metadata":{"id":"FAa6JI1PJCxN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip flickr8k.zip;"],"metadata":{"id":"Y5v3BtO4K1J5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os  # when loading file paths\n","import pandas as pd  # for lookup in annotation file\n","import spacy  # for tokenizer\n","import torch\n","from torch.nn.utils.rnn import pad_sequence  # pad batch\n","from torch.utils.data import DataLoader, Dataset\n","from PIL import Image  # Load img\n","import torchvision.transforms as transforms\n","import string\n","import torch\n","import torch.nn as nn\n","import statistics\n","import torchvision.models as models\n","import torch.optim as optim\n","import tqdm"],"metadata":{"id":"VekvGiwoDepZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transform = transforms.Compose(\n","    [\n","        transforms.Resize((299,299)),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","    ]\n",")"],"metadata":{"id":"3GTxmfdsDeh8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["spacy_eng = spacy.load(\"en_core_web_sm\")"],"metadata":{"id":"_3wBkxCyEVnY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Vocabulary:\n","    def __init__(self, freq_threshold):\n","        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n","        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n","        self.freq_threshold = freq_threshold\n","\n","    def __len__(self):\n","        return len(self.itos)\n","\n","    @staticmethod\n","\n","    def tokenizer_eng(text):\n","\n","      rem_punct = str.maketrans('', '', string.punctuation)\n","      tokens=[tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n","      tokens = [tok.translate(rem_punct) for tok in tokens]\n","      tokens = [tok for tok in tokens if len(tok) > 1]\n","      # remove numeric values\n","      tokens = [tok for tok in tokens if tok.isalpha()]\n","\n","      return tokens\n","\n","\n","    def build_vocabulary(self, sentence_list):\n","        frequencies = {}\n","        idx = 4\n","\n","        for sentence in sentence_list:\n","            for word in self.tokenizer_eng(sentence):\n","                if word not in frequencies:\n","                    frequencies[word] = 1\n","\n","                else:\n","                    frequencies[word] += 1\n","\n","                if frequencies[word] == self.freq_threshold:\n","                    self.stoi[word] = idx\n","                    self.itos[idx] = word\n","                    idx += 1\n","        print('lenth of vacbulary',len(self.stoi.keys()),len(self.itos.keys()))\n","\n","\n","    def numericalize(self, text):\n","        tokenized_text = self.tokenizer_eng(text)\n","\n","        return [\n","            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n","            for token in tokenized_text\n","        ]"],"metadata":{"id":"Sb_l22FxEM0y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class FlickrDataset(Dataset):\n","    def __init__(self, root_dir, captions_file, transform=None, freq_threshold=5):\n","        self.root_dir = root_dir\n","        self.df = pd.read_csv(captions_file)\n","        self.transform = transform\n","\n","        # Get img, caption columns\n","        self.imgs = self.df[\"image\"]\n","        self.captions = self.df[\"caption\"]\n","\n","        # Initialize vocabulary and build vocab\n","        self.vocab = Vocabulary(freq_threshold)\n","        self.vocab.build_vocabulary(self.captions.tolist())\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, index):\n","        caption = self.captions[index]\n","        img_id = self.imgs[index]\n","        img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n","\n","        if self.transform is not None:\n","            img = self.transform(img)\n","\n","        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n","        numericalized_caption += self.vocab.numericalize(caption)\n","        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n","\n","        return img, torch.tensor(numericalized_caption)"],"metadata":{"id":"IOdV5Uu-DcYG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = FlickrDataset('/content/Images', '/content/captions.txt', transform=transform)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BGp6-LD8DksP","executionInfo":{"status":"ok","timestamp":1684161705494,"user_tz":-330,"elapsed":2221,"user":{"displayName":"Rishabh Dwivedi","userId":"11333895928926712569"}},"outputId":"2e471a94-a1a2-4e94-98c1-64c27dbc7cc2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["lenth of vacbulary 2970 2970\n"]}]},{"cell_type":"code","source":["# dataset[0]"],"metadata":{"id":"vujXqyorK_Ml"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MyCollate:\n","    def __init__(self, pad_idx):\n","        self.pad_idx = pad_idx\n","\n","    def __call__(self, batch):\n","        imgs = [item[0].unsqueeze(0) for item in batch]\n","        imgs = torch.cat(imgs, dim=0)\n","        targets = [item[1] for item in batch]\n","        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)\n","        return imgs, targets\n"],"metadata":{"id":"HmU57jOLDkfz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n","\n","loader = DataLoader(\n","    dataset=dataset,\n","    batch_size=32,\n","    num_workers=8,\n","    shuffle=True,\n","    pin_memory=True,\n","    collate_fn=MyCollate(pad_idx=pad_idx),\n",")\n"],"metadata":{"id":"q1L9tMK1E00V","executionInfo":{"status":"ok","timestamp":1684161705496,"user_tz":-330,"elapsed":8,"user":{"displayName":"Rishabh Dwivedi","userId":"11333895928926712569"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7adbde28-030f-48b0-d633-a87d2cfaeb4a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]}]},{"cell_type":"code","source":["import pickle\n","\n","\n","with open(\"vocab_itos.txt\", \"wb\") as file:\n","    pickle.dump(dataset.vocab.itos, file)\n"],"metadata":{"id":"R07Z0nHE_i5A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset.vocab.itos"],"metadata":{"id":"t9T7Rc0CLIR2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n"],"metadata":{"id":"bozHWFfJDy7F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class EncoderCNN(nn.Module):\n","    def __init__(self, embed_size, train_CNN=False):\n","        super(EncoderCNN, self).__init__()\n","        self.train_CNN = train_CNN\n","        self.inception = models.inception_v3(pretrained=True, aux_logits=True)\n","        self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size)\n","        self.relu = nn.ReLU()\n","        # self.times = []\n","        self.dropout = nn.Dropout(0.2)\n","\n","    def forward(self, images):\n","        features = self.inception(images)\n","        \n","        return self.dropout(self.relu(features[0]))\n"],"metadata":{"id":"U1tNStf-FNV1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DecoderRNN(nn.Module):\n","    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n","        super(DecoderRNN, self).__init__()\n","        self.embed = nn.Embedding(vocab_size, embed_size)\n","        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n","        self.linear = nn.Linear(hidden_size, vocab_size)\n","        self.dropout = nn.Dropout(0.2)\n","\n","    def forward(self, features, captions):\n","        embeddings = self.dropout(self.embed(captions))\n","        embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0)\n","        hiddens, _ = self.lstm(embeddings)\n","        outputs = self.linear(hiddens)\n","        return outputs\n"],"metadata":{"id":"JOXy-rNeFrGJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CNNtoRNN(nn.Module):\n","    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n","        super(CNNtoRNN, self).__init__()\n","        self.encoderCNN = EncoderCNN(embed_size)\n","        self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n","\n","    def forward(self, images, captions):\n","        features = self.encoderCNN(images)\n","        outputs = self.decoderRNN(features, captions)\n","        return outputs\n","\n","    def caption_image(self, image, vocabulary, max_length=50):\n","        result_caption = []\n","\n","        with torch.no_grad():\n","            x = self.encoderCNN(image).unsqueeze(0)\n","\n","            print('shape of ecoded image',x.shape)\n","            states = None\n","\n","            for _ in range(max_length):\n","                hiddens, states = self.decoderRNN.lstm(x, states)\n","                output = self.decoderRNN.linear(hiddens.squeeze(0))\n","                print('predicted-----',output)\n","                print('predicted shape-----',output.shape)\n","\n","                predicted = output.argmax()\n","                result_caption.append(predicted.item())\n","                x = self.decoderRNN.embed(predicted).unsqueeze(0)\n","\n","                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n","                    break\n","                    \n","        return [vocabulary.itos[idx] for idx in result_caption]"],"metadata":{"id":"JjF0X6BtFrAW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset.vocab"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AIVBxtZp-gQJ","executionInfo":{"status":"ok","timestamp":1684163667282,"user_tz":-330,"elapsed":597,"user":{"displayName":"Rishabh Dwivedi","userId":"11333895928926712569"}},"outputId":"99df061e-ff06-4207-81bf-9c77b8257dcb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<__main__.Vocabulary at 0x7fa4a2d430d0>"]},"metadata":{},"execution_count":94}]},{"cell_type":"code","source":["len(dataset.vocab)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JaBZ_zEk3De9","executionInfo":{"status":"ok","timestamp":1684162748067,"user_tz":-330,"elapsed":8,"user":{"displayName":"Rishabh Dwivedi","userId":"11333895928926712569"}},"outputId":"9e0bd4d5-66a9-4169-e5f0-bd37f94c577b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2970"]},"metadata":{},"execution_count":75}]},{"cell_type":"code","source":["embed_size = 256\n","hidden_size = 256\n","vocab_size = len(dataset.vocab)\n","num_layers = 1\n","learning_rate = 3e-4\n","num_epochs = 100"],"metadata":{"id":"Ddce_1ZyXuFq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"OSbnJn-2Zq-P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(train_loader,embed_size,hidden_size,vocab_size,num_layers):\n","\n","    torch.backends.cudnn.benchmark = True\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","\n","    # initialize model, loss etc\n","    model = CNNtoRNN(embed_size, hidden_size, vocab_size, num_layers).to(device)\n","    criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # Only finetune the CNN\n","    for name, param in model.encoderCNN.inception.named_parameters():\n","        if \"fc.weight\" in name or \"fc.bias\" in name:\n","            param.requires_grad = True\n","        else:\n","            param.requires_grad = False\n","\n","    model.train()\n","\n","    epoch_loss =float('inf')\n","\n","    for epoch in range(num_epochs):\n","        # Uncomment the line below to see a couple of test cases\n","        # print_examples(model, device, dataset)\n","\n","        for idx, (imgs, captions) in tqdm.tqdm(\n","            enumerate(train_loader), total=len(train_loader), leave=False\n","        ):\n","            imgs = imgs.to(device)\n","            captions = captions.to(device)\n","\n","            outputs = model(imgs, captions[:-1])\n","            loss = criterion(\n","                outputs.reshape(-1, outputs.shape[2]), captions.reshape(-1)\n","            )\n","        \n","            optimizer.zero_grad()\n","            loss.backward(loss)\n","            optimizer.step()\n","\n","        print(f'Loss: {epoch:.4f} Acc: {loss:.4f}')\n","\n","        if loss<epoch_loss:\n","          epoch_loss = loss\n","          torch.save(model.state_dict(), '/content/drive/MyDrive/image_caption/best.pth')\n","\n","\n","    torch.save(model.state_dict(), '/content/drive/MyDrive/image_caption/last.pth')\n"],"metadata":{"id":"8GnZDIOhbWQW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train(loader,embed_size,hidden_size,vocab_size,num_layers)"],"metadata":{"id":"2LV-q4GjjjvV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Load and Test"],"metadata":{"id":"Xs-__DURov_v"}},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"i_YdtlrB3IHl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = CNNtoRNN(embed_size, hidden_size, vocab_size, num_layers).to(device)"],"metadata":{"id":"HZQCrwNZ1ewL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.to(device)"],"metadata":{"id":"ahecU2kA1mdL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.load_state_dict(torch.load('/content/drive/MyDrive/image_caption/last.pth'))\n","# model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F2qq_q2c3rdE","executionInfo":{"status":"ok","timestamp":1684166211728,"user_tz":-330,"elapsed":2628,"user":{"displayName":"Rishabh Dwivedi","userId":"11333895928926712569"}},"outputId":"c452bd52-a2b7-402e-e497-7e579bc47ec3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":105}]},{"cell_type":"code","source":["def print_examples(model, device, dataset,img_path):\n","    transform = transforms.Compose(\n","        [\n","            transforms.Resize((299, 299)),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","        ]\n","    )\n","\n","    model.eval()\n","    test_img1 = transform(Image.open(img_path).convert(\"RGB\")).unsqueeze(\n","        0\n","    )\n","\n","    print(\"Example 1 CORRECT: Dog on a beach by the ocean\",test_img1.shape)\n","    print(\n","        \"Example 1 OUTPUT: \"\n","        + \" \".join(model.caption_image(test_img1.to(device), dataset.vocab))\n","    )\n"],"metadata":{"id":"A_8vCzQijt4u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img_path='/content/1024138940_f1fefbdce1.jpg'"],"metadata":{"id":"7rOk1rT34QJX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a=torch.tensor([-3.9508, 39.9415, -2.7044]).argmax()\n","a"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fNFwAKaw6S9E","executionInfo":{"status":"ok","timestamp":1684166219763,"user_tz":-330,"elapsed":9,"user":{"displayName":"Rishabh Dwivedi","userId":"11333895928926712569"}},"outputId":"2d1b5de2-1bd7-4fb9-c2f0-bc13892ffb51"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1)"]},"metadata":{},"execution_count":108}]},{"cell_type":"code","source":["print_examples(model,device,dataset,img_path)"],"metadata":{"id":"r0JVUUsF4GXb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yuuYjWJB4U_A"},"execution_count":null,"outputs":[]}]}